{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据拼接\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib as mpl\n",
    "from collections import Counter\n",
    "\n",
    "# 忽略所有警告\n",
    "warnings.filterwarnings('ignore')\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['AR PL UKai CN']  # 用来正常显示中文标签 # 选择已安装的中文字体，这里以 'DejaVu Sans' 为例\n",
    "plt.rcParams['axes.unicode_minus'] = True  # 解决负号显示问题\n",
    "# mpl.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "#将数据进行分类\n",
    "def data_split(dataframe, random_state, test_size=0.3): #0.2是测试集的比例，用于后面测试的！！！\n",
    "    # the columns of dataframe: names, labels, radiomics_features...\n",
    "    X = dataframe.iloc[:,1:]  # 从第2列开始的特征，具体根据自己的数据改\n",
    "    Y = dataframe['label'] #填自己的标签\n",
    "    # 使用分层采样\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state, stratify=Y)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X, Y\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假设数据存储在DataFrame中\n",
    "A = './final_stats/retrospective.xlsx'  # 替换为您的文件路径\n",
    "# file_path = './final_stats/prospective.xlsx'  # 替换为您的文件路径\n",
    "\n",
    "data = pd.read_excel(A)  # 读取Excel文件\n",
    "# data = data.dropna()\n",
    "columns_to_keep = ['AGE', 'BMI', 'tPSA', 'fPSA', 'f/t', 'p2PSA',\n",
    "                   'PHI', 'Volume', 'PSAD', 'PHID', 'Blood Neutrophil',\n",
    "                   'Blood Lymphocyte', 'NLR', 'PI-RADS', 'Urinary leukocytes']\n",
    "\n",
    "X_A = data[columns_to_keep].round(4)\n",
    "y_A = data['SPC']\n",
    "# print(X_A.head())\n",
    "# X_raw_train, X_raw_test, Y_train, Y_test = train_test_split(X_A, y_A, test_size=0.3, random_state=2024, stratify=y_A)\n",
    "\n",
    "B = './final_stats/prospective.xlsx'  \n",
    "data = pd.read_excel(B)\n",
    "data = data.dropna()\n",
    "X_B = data[columns_to_keep].round(4)\n",
    "y_B = data['SPC']\n",
    "# print(X_B.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.read_excel(A).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 标准化特征\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # 归一化特征\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_features_train = scaler.fit_transform(X_A)\n",
    "# X_train = pd.DataFrame(scaled_features_train, columns=X_A.columns)\n",
    "Y_train = y_A\n",
    "# X_test = pd.DataFrame(scaler.transform(X_B), columns=X_B.columns)\n",
    "Y_test = y_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义混合类型特征预处理\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# 定义特征类型\n",
    "continuous_features = ['AGE', 'BMI', 'tPSA', 'fPSA', 'p2PSA',\n",
    "                      'PHI', 'Volume', 'PSAD', 'PHID',\n",
    "                      'Blood Neutrophil', 'Blood Lymphocyte','f/t', 'NLR']\n",
    "non_trans_features = ['Urinary leukocytes', 'PI-RADS']\n",
    "# ordinal_features = ['PI-RADS']\n",
    "# 构建预处理Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('continuous', MinMaxScaler(), continuous_features),\n",
    "                ('non-trans', 'passthrough', non_trans_features),\n",
    "                # ('ordinal', 'passthrough', ordinal_features)  # 保留有序编码\n",
    "])\n",
    "X_train = preprocessor.fit_transform(X_A)\n",
    "# 将结果转换为 DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train, columns=continuous_features + non_trans_features)\n",
    "# 确保列的顺序和原始数据一致\n",
    "X_train = X_train_scaled_df[continuous_features + non_trans_features]\n",
    "X_test = preprocessor.transform(X_B)\n",
    "X_test_scaled_df = pd.DataFrame(X_test, columns=continuous_features + non_trans_features)\n",
    "X_test = X_test_scaled_df[continuous_features + non_trans_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "index=  X_train.columns\n",
    "alphas = np.logspace(-7,1,100)\n",
    "max_iter = 100000\n",
    "model_lassoCV = LassoCV(alphas=alphas, max_iter=max_iter).fit(X_train, Y_train)\n",
    "coefs = model_lassoCV.path(X_train, Y_train, alphas=alphas, max_iter=max_iter)[1].T\n",
    "# #为True的时候打开这行代码,false就注释掉\n",
    "# coefs = coefs.reshape(coefs.shape[0], (coefs.shape[1]*coefs.shape[2]))\n",
    "# print(coefs.shape)\n",
    "alphas = model_lassoCV.alpha_\n",
    "print(\"LassoCV回归后的最佳参数值\",alphas)\n",
    "\n",
    "# 计算LassoCV后剩下的非零特征权重\n",
    "tmp = pd.Series(model_lassoCV.coef_, index=index)\n",
    "print(model_lassoCV.coef_)\n",
    "featureImportance = tmp[tmp!=0]\n",
    "featureImportance.sort_values(ascending=True,inplace=True)\n",
    "print(\"LassoCV后的特征数量为\",featureImportance.shape[0])\n",
    "print(\"对应的特征名称包括\",featureImportance.index.values.tolist())\n",
    "print()\n",
    "\n",
    "# 图例输出\n",
    "# 可视化LassoCV过程\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.semilogx(model_lassoCV.alphas_,coefs, '-')\n",
    "plt.axvline(model_lassoCV.alpha_,color = 'gray',ls=\"--\")\n",
    "plt.tick_params(axis='x', labelsize=20)  # 设置X轴刻度\n",
    "plt.tick_params(axis='y', labelsize=20)  # 设置Y轴刻度\n",
    "plt.xlabel('正则化参数',fontsize=20)\n",
    "plt.ylabel('回归系数',fontsize=20)\n",
    "plt.savefig(\"./final_stats/02LassoCV_01.png\",dpi=600,format='eps')\n",
    "\n",
    "# 可视化非零特征的权重值\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(featureImportance.index,featureImportance)\n",
    "plt.title('Relative Importance of Features',fontdict={'size': 20})\n",
    "plt.tight_layout()\n",
    "plt.tick_params(axis='x', labelsize=20)  # 设置X轴刻度\n",
    "plt.tick_params(axis='y', labelsize=20)  # 设置Y轴刻度\n",
    "plt.savefig(\"./final_stats/02LassoCV_02.png\",dpi=600,format='eps')\n",
    "\n",
    "# 可视化不同Lambda值对应的MSE\n",
    "MSE = model_lassoCV.mse_path_\n",
    "MSE_mean = np.mean(MSE,axis=1)\n",
    "MSE_std = np.std(MSE,axis=1)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.errorbar(model_lassoCV.alphas_,MSE_mean        #x, y数据，一一对应\n",
    "                , yerr=MSE_std                     #y误差范围\n",
    "                , fmt=\"o\"                          #数据点标记\n",
    "                , ms=2                             #数据点大小\n",
    "                , mfc=\"g\"                          #数据点颜色\n",
    "                , mec=\"r\"                          #数据点边缘颜色\n",
    "                , ecolor=\"lightblue\"               #误差棒颜色\n",
    "                , elinewidth=2                     #误差棒线宽\n",
    "                , capsize=4                        #误差棒边界线长度\n",
    "                , capthick=1)                      #误差棒边界线厚度\n",
    "plt.semilogx()\n",
    "plt.axvline(model_lassoCV.alpha_,color = 'gray',ls=\"--\")\n",
    "plt.xlabel('正则化参数',fontdict={'size': 20})\n",
    "plt.ylabel('均方误差',fontdict={'size': 20})\n",
    "plt.tick_params(axis='x', labelsize=20)  # 设置X轴刻度\n",
    "plt.tick_params(axis='y', labelsize=20)  # 设置Y轴刻度\n",
    "plt.savefig(\"./final_stats/02LassoCV_03.png\",dpi=600,format='eps')\n",
    "\n",
    "\n",
    "# 获取筛选后的临床数据（训练集和测试集）\n",
    "tmp = pd.Series(model_lassoCV.coef_, index=index)\n",
    "featureImportance = tmp[tmp!=0]\n",
    "featureImportance.sort_values(ascending=True,inplace=True)\n",
    "X_train = X_train.loc[:,featureImportance.index]#根据选取需要的特征\n",
    "X_test =X_test.loc[:,featureImportance.index]\n",
    "# X = X.loc[:,featureImportance.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# 定义参数网格\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 0.3, 0.7]\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 0.3, 0.7]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'saga']  # 根据penalty动态匹配\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'poly', 'rbf'],\n",
    "        'gamma': ['scale', 0.1, 1]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'colsample_bytree': [0.5, 0.7, 1]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200],\n",
    "        'depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'MLP': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50,50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.0001, 0.001, 0.01]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 创建模型实例\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=2024),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=2024),\n",
    "    # 'KNN': KNeighborsClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(random_state=2024, max_iter=1000),\n",
    "    'SVM': SVC(random_state=2024, probability=True),\n",
    "    'XGBoost': XGBClassifier(random_state=2024, verbosity=0),\n",
    "    'CatBoost': CatBoostClassifier(random_state=2024, verbose=0),\n",
    "    # 'GradientBoosting': GradientBoostingClassifier(random_state=2024),\n",
    "    'MLP': MLPClassifier(max_iter=500, random_state=2024)\n",
    "}\n",
    "\n",
    "# 用于存储最佳参数和分数\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "# 定义一个使用宏平均F1分数的评分器\n",
    "# macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# 使用StratifiedKFold进行分层交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, random_state=2024, shuffle=True)\n",
    "print(X_train.shape)\n",
    "# 对每个模型进行GridSearchCV\n",
    "for model_name in models.keys():\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=models[model_name], param_grid=param_grids[model_name], cv=cv, n_jobs=4)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    best_scores[model_name] = grid_search.best_score_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {grid_search.best_score_}\\n\")\n",
    "\n",
    "# 输出所有模型的最佳参数和分数\n",
    "for model_name in best_params.keys():\n",
    "    print(f\"{model_name} - Best Parameters: {best_params[model_name]}\")\n",
    "    print(f\"{model_name} - Best Score: {best_scores[model_name]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型池\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 机器学习分类模型建模\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils import compute_sample_weight\n",
    "random_seed = 2024\n",
    "n_classes = [0, 1]\n",
    "\n",
    "print(best_params)\n",
    "# best_params = {'RandomForest': {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}, 'DecisionTree': {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 8, 'min_samples_split': 2}, 'KNN': {'algorithm': 'auto', 'n_neighbors': 7, 'weights': 'distance'}, 'LogisticRegression': {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}, 'SVM': {'C': 100, 'gamma': 'auto', 'kernel': 'rbf'}, 'XGBoost': {'colsample_bytree': 1, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}, 'CatBoost': {'depth': 8, 'iterations': 100, 'learning_rate': 0.1}, 'GradientBoosting': {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}, 'MLP': {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant', 'solver': 'adam'}}\n",
    "# best_params = {'RandomForest': {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100},\n",
    "#             'DecisionTree': {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2},\n",
    "#             'KNN': {'algorithm': 'auto', 'n_neighbors': 9, 'weights': 'uniform'},\n",
    "#             'LogisticRegression': {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'},\n",
    "#             'SVM': {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "#             'XGBoost': {'colsample_bytree': 1, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50},\n",
    "#             'CatBoost': {'depth': 6, 'iterations': 100, 'learning_rate': 0.1},\n",
    "#             'GradientBoosting': {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200},\n",
    "#             'MLP': {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}}\n",
    "\n",
    "# 使用最佳参数初始化模型\n",
    "rf_model = RandomForestClassifier(**best_params['RandomForest'], random_state=2024)\n",
    "dt_model = DecisionTreeClassifier(**best_params['DecisionTree'], random_state=2024)\n",
    "# knn_model = KNeighborsClassifier(**best_params['KNN'])\n",
    "lr_model = LogisticRegression(**best_params['LogisticRegression'], random_state=2024, max_iter=1000)\n",
    "svm_model = SVC(**best_params['SVM'], random_state=2024, probability=True)  \n",
    "xgb_model = XGBClassifier(**best_params['XGBoost'], random_state=2024, verbosity=0)\n",
    "cat_model = CatBoostClassifier(**best_params['CatBoost'], random_state=2024, verbose=0)\n",
    "# gb_model = GradientBoostingClassifier(**best_params['GradientBoosting'], random_state=2024)\n",
    "mlp_model = MLPClassifier(**best_params['MLP'], random_state=2024, max_iter=1000)\n",
    "tuned_models = {\n",
    "    'RandomForest': RandomForestClassifier(**best_params['RandomForest'], random_state=2024),\n",
    "    'DecisionTree': DecisionTreeClassifier(**best_params['DecisionTree'], random_state=2024),\n",
    "    # 'KNN': KNeighborsClassifier(**best_params['KNN']),\n",
    "    'LogisticRegression': LogisticRegression(**best_params['LogisticRegression'], random_state=2024, max_iter=1000),\n",
    "    'SVM':SVC(**best_params['SVM'], random_state=2024, probability=True)  ,\n",
    "    'XGBoost': XGBClassifier(**best_params['XGBoost'], random_state=2024, verbosity=0),\n",
    "    'CatBoost': CatBoostClassifier(**best_params['CatBoost'], random_state=2024, verbose=0),\n",
    "    # 'GradientBoosting': GradientBoostingClassifier(**best_params['GradientBoosting'], random_state=2024),\n",
    "    'MLP': MLPClassifier(**best_params['MLP'], random_state=2024, max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.interpolate import interp1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "def compute_bootstrap_ci(metric_func, y_true, y_pred, y_proba=None, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    使用 bootstrap 方法计算给定指标的置信区间.\n",
    "    \n",
    "    参数:\n",
    "        metric_func: 评估指标函数（如 accuracy_score, f1_score 等）\n",
    "        y_true: 测试集的真实标签\n",
    "        y_pred: 测试集的预测标签\n",
    "        y_proba: 测试集的预测概率（如果需要计算 AUC）\n",
    "        n_bootstraps: 重采样的次数\n",
    "        alpha: 显著性水平（0.05 表示 95% 置信区间）\n",
    "    \n",
    "    返回:\n",
    "        均值, (下限, 上限) 的置信区间\n",
    "    \"\"\"\n",
    "    bootstrapped_scores = []\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    # 引入重采样的部分\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = resample(np.arange(n_samples), replace=True, n_samples=n_samples)\n",
    "        \n",
    "        # 使用 iloc 来索引\n",
    "        y_true_sample = y_true.iloc[indices]\n",
    "        # y_pred_sample = y_pred.iloc[indices]\n",
    "        y_pred_sample = y_pred[indices]\n",
    "\n",
    "        # 如果有 y_proba\n",
    "        if y_proba is not None:\n",
    "            y_proba_sample = y_proba[indices]\n",
    "\n",
    "        # 计算指标\n",
    "        score = metric_func(y_true_sample, y_pred_sample)\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    \n",
    "    # 计算均值和置信区间\n",
    "    bootstrapped_scores = np.array(bootstrapped_scores)\n",
    "    lower = round(np.percentile(bootstrapped_scores, 100 * alpha / 2), 3)\n",
    "    upper = round(np.percentile(bootstrapped_scores, 100 * (1 - alpha / 2)), 3)\n",
    "    mean_score = np.mean(bootstrapped_scores)\n",
    "    return mean_score, (lower, upper)\n",
    "\n",
    "# 定义计算特异度的函数\n",
    "def specificity_metric(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm[0, 0]  # True negatives\n",
    "    fp = cm[0, 1]  # False positives\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# 定义计算特异度的函数\n",
    "def sensitivity_metric(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp = cm[1, 1]  # True positives\n",
    "    fn = cm[1, 0]  # False negatives\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def ppv_metric(y_true, y_pred):\n",
    "    \"\"\"计算 Positive Predictive Value (PPV)\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "def npv_metric(y_true, y_pred):\n",
    "    \"\"\"计算 Negative Predictive Value (NPV)\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN = cm[0][0]\n",
    "    FN = cm[1][0]\n",
    "    return TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def delong_ci(y_true, y_proba, alpha=0.95):\n",
    "    \"\"\"使用 DeLong 方法计算 AUC 的置信区间\"\"\"\n",
    "    # auc = roc_auc_score(y_true, y_proba)\n",
    "    # n1 = np.sum(y_true == 1)  # 正类样本数\n",
    "    # n0 = len(y_true) - n1     # 负类样本数\n",
    "\n",
    "    # # DeLong 变异计算\n",
    "    # v10 = np.var(y_proba[y_true == 1]) / n1\n",
    "    # v01 = np.var(y_proba[y_true == 0]) / n0\n",
    "    # se = np.sqrt(v10 + v01)\n",
    "\n",
    "    # # 计算 Z 值\n",
    "    # z = norm.ppf(1 - (1 - alpha) / 2)\n",
    "    # lower = max(0, auc - z * se)\n",
    "    # upper = min(1, auc + z * se)\n",
    "    # return auc, (lower, upper)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    n1 = np.sum(y_true == 1)  # 正类样本数\n",
    "    n0 = len(y_true) - n1     # 负类样本数\n",
    "    \n",
    "    # DeLong方差计算\n",
    "    v10 = 1 / n1 * np.sum((y_proba[y_true == 1] - auc) ** 2)\n",
    "    v01 = 1 / n0 * np.sum((y_proba[y_true == 0] - auc) ** 2)\n",
    "    se = np.sqrt((v10 / n1) + (v01 / n0))\n",
    "    \n",
    "    # 计算Z值\n",
    "    z = norm.ppf(1 - (1 - alpha) / 2)\n",
    "    lower = auc - z * se\n",
    "    upper = auc + z * se\n",
    "    return auc, (max(0, lower), min(1, upper))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('RandomForest', rf_model),\n",
    "    ('LogisticRegression', lr_model),\n",
    "    ('SVM', svm_model),\n",
    "    ('XGBoost', xgb_model),\n",
    "    # ('CatBoost', cat_model),\n",
    "    ('MLP', mlp_model)\n",
    "]\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "tuned_models['stacking'] = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=5)\n",
    "tuned_models['voting'] = VotingClassifier(estimators=base_models, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "n_classes = 2  # 二分类问题\n",
    "\n",
    "# 初始化存储每个模型结果的字典\n",
    "train_results = {}\n",
    "\n",
    "# 初始化存储每个折叠结果的列表\n",
    "train_tprs_micro = []\n",
    "train_aucs_micro = []\n",
    "train_mean_fpr_micro = np.linspace(0, 1, 100)\n",
    "train_aucs_95ci = []\n",
    "train_conf_matrices = []\n",
    "\n",
    "# 评估模型在测试集上的性能\n",
    "for model_name, model in tuned_models.items():\n",
    "    model.fit(X_train, Y_train)\n",
    "    # 预测测试集上的概率\n",
    "    y_proba_train = model.predict_proba(X_train)\n",
    "    \n",
    "\n",
    "    # 计算ROC曲线和AUC\n",
    "    fpr, tpr, _ = roc_curve(Y_train, y_proba_train[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # 计算 micro 平均的ROC曲线和AUC\n",
    "    interp_tpr_micro = np.interp(train_mean_fpr_micro, fpr, tpr)\n",
    "    interp_tpr_micro[0] = 0.0  # 初始点为0\n",
    "    train_tprs_micro.append(interp_tpr_micro)\n",
    "    train_aucs_micro.append(roc_auc)\n",
    "    \n",
    "    # print(model.predict_proba(X_train))\n",
    "    # 设定新的阈值\n",
    "    threshold = 0.5\n",
    "\n",
    "    # 使用新的阈值进行预测\n",
    "    y_pred_train = (y_proba_train[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # 计算训练集上的性能指标\n",
    "    # y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = accuracy_score(Y_train, y_pred_train)\n",
    "    f1_train = f1_score(Y_train, y_pred_train, average='weighted')\n",
    "\n",
    "    # 计算测试集上的混淆矩阵\n",
    "    cm = confusion_matrix(Y_train, y_pred_train, labels=np.unique(Y_train))\n",
    "    train_conf_matrices.append(cm)\n",
    "\n",
    "    # 计算sensitivity和specificity    \n",
    "    TP = cm[1][1]  # 真阳性的数量\n",
    "    FN = cm[1][0]  # 假阴性的数量\n",
    "    TN = cm[0][0]  # 真阴性的数量\n",
    "    FP = cm[0][1]  # 假阳性的数量\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    npv = TN / (TN + FN) if (TP + FN) != 0 else 0\n",
    "\n",
    "    # 计算95%置信区间\n",
    "    accuracy_ci = compute_bootstrap_ci(accuracy_score, Y_train, y_pred_train)\n",
    "    f1_ci = compute_bootstrap_ci(lambda y, y_hat: f1_score(y, y_hat, average='weighted'), Y_train, y_pred_train)\n",
    "    auc_ci = compute_bootstrap_ci(roc_auc_score, Y_train, y_proba_train[:, 1])\n",
    "\n",
    "    sensitivity_ci = compute_bootstrap_ci(sensitivity_metric, Y_train, y_pred_train)\n",
    "\n",
    "    specificity_ci = compute_bootstrap_ci(specificity_metric, Y_train, y_pred_train)\n",
    "    ppv_mean, ppv_ci = compute_bootstrap_ci(ppv_metric, Y_train, y_pred_train)\n",
    "    # print(f\"PPV: {ppv_mean}, 95%CI: {ppv_ci}\")\n",
    "\n",
    "    # 计算 NPV 的置信区间\n",
    "    npv_mean, npv_ci = compute_bootstrap_ci(npv_metric, Y_train, y_pred_train)\n",
    "\n",
    "    train_aucs_95ci.append(auc_ci[1])\n",
    "\n",
    "    # 输出模型的性能指标\n",
    "    print(f'{model_name} (Internal Test Set):')\n",
    "    # 输出指标及其95%置信区间\n",
    "    print(f'  Accuracy = {accuracy_train:.3f}, 95%CI: {accuracy_ci[1]}')\n",
    "    print(f'  AUC (micro) = {roc_auc:.3f}, 95%CI: {auc_ci[1]}')\n",
    "    print(f'  F1-score = {f1_train:.3f}, 95%CI: {f1_ci[1]}')\n",
    "    print(f'  Mean Sensitivity = {sensitivity_ci[0]:.3f}, 95%CI: {sensitivity_ci[1]}')\n",
    "    print(f'  Mean Specificity = {specificity_ci[0]:.3f}, 95%CI: {specificity_ci[1]}')\n",
    "    print(f'  PPV = {ppv:.3f}, 95%CI: {ppv_ci}')\n",
    "    print(f'  NPV = {npv:.3f}, 95%CI: {npv_ci}')\n",
    "    \n",
    "    # 存储结果\n",
    "    train_results[model_name] = {\n",
    "        'accuracy': accuracy_train,\n",
    "        'auc_micro': roc_auc,\n",
    "        'f1_score': f1_train,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv,\n",
    "    }\n",
    "    \n",
    "# 绘制 micro 平均的ROC曲线图\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, model_name in enumerate(tuned_models.keys()):\n",
    "    plt.plot(train_mean_fpr_micro, train_tprs_micro[i], lw=2, label=f'{model_name} (AUC = {train_aucs_micro[i]:.3f} 95%CI: {train_aucs_95ci[i]})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (Retrospective Set)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim(0, 1)  # 设置 x 轴范围\n",
    "plt.ylim(0, 1)  # 设置 y 轴范围\n",
    "plt.show()\n",
    "\n",
    "# 绘制每个模型的混淆矩阵图像\n",
    "n_models = len(tuned_models)  # 模型数量\n",
    "n_cols = 5  # 每行显示的模型数量\n",
    "n_rows = (n_models + n_cols - 1) // n_cols  # 计算行数\n",
    "\n",
    "# plt.figure(figsize=(20, 4 * n_rows))  # 调整整体图像大小\n",
    "\n",
    "# for i, (model_name, conf_matrix) in enumerate(zip(tuned_models.keys(), train_conf_matrices)):\n",
    "#     plt.subplot(n_rows, n_cols, i + 1)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.unique(Y_train))\n",
    "#     disp.plot(cmap=plt.cm.Blues, ax=plt.gca(), colorbar=False)\n",
    "#     plt.title(f'Confusion Matrix for {model_name}')\n",
    "#     plt.xlabel('')  # 去掉x轴标签\n",
    "#     plt.ylabel('')  # 去掉y轴标签\n",
    "\n",
    "# plt.tight_layout()  # 调整子图间的布局以避免重叠\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "# 绘制主图\n",
    "plt.figure(figsize=(8, 8),dpi=600)\n",
    "for i, model_name in enumerate(tuned_models.keys()):\n",
    "    plt.plot(train_mean_fpr_micro, train_tprs_micro[i], lw=2,\n",
    "             label=f'{model_name} (AUC = {train_aucs_micro[i]:.3f} 95%CI: {train_aucs_95ci[i][0]:.3f}-{train_aucs_95ci[i][1]:.3f})',)\n",
    "\n",
    "\n",
    "# 主图设置\n",
    "plt.xlabel('假阳性率')\n",
    "plt.ylabel('真正率')\n",
    "plt.title('ROC 曲线 (回顾性队列)')\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "# 添加放大子图（调整位置和大小）\n",
    "ax_inset = inset_axes(plt.gca(), width=\"40%\", height=\"40%\", loc='lower right',\n",
    "                      bbox_to_anchor=(-0.32, 0.3,1.3,1.3),  # 控制位置和大小\n",
    "                      bbox_transform=plt.gca().transAxes)\n",
    "# 添加放大区域的边框线\n",
    "# plt.gca().indicate_inset_zoom(ax_inset, edgecolor=\"gray\")\n",
    "# 绘制放大区域的曲线（限定坐标范围）\n",
    "for i, model_name in enumerate(tuned_models.keys()):\n",
    "    ax_inset.plot(train_mean_fpr_micro, train_tprs_micro[i], lw=1.5, alpha=0.8)\n",
    "# 设置放大区域的坐标范围（示例：FPR 0~0.2，TPR 0.8~1）\n",
    "ax_inset.set_xlim(0, 0.2)\n",
    "ax_inset.set_ylim(0.8, 1.0)\n",
    "ax_inset.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_models = [\n",
    "#     ('RandomForest', RandomForestClassifier(**best_params['RandomForest'], random_state=2024)),\n",
    "#     ('LogisticRegression', LogisticRegression(**best_params['LogisticRegression'], random_state=2024, max_iter=1000)),\n",
    "#     ('SVM', SVC(**best_params['SVM'], random_state=2024, probability=True)),\n",
    "#     ('XGBoost', XGBClassifier(**best_params['XGBoost'], random_state=2024, verbosity=0)),\n",
    "#     ('CatBoost', CatBoostClassifier(**best_params['CatBoost'], random_state=2024, verbose=0)),\n",
    "#     ('MLP', MLPClassifier(**best_params['MLP'], random_state=2024, max_iter=1000))\n",
    "# ]\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# tuned_models['stacking'] = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=5)\n",
    "# tuned_models['voting'] = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "# tuned_models['stacking'].fit(X_train, Y_train)\n",
    "# tuned_models['voting'].fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "n_classes = 2  # 二分类问题\n",
    "\n",
    "# 初始化存储每个模型结果的字典\n",
    "test_results = {}\n",
    "\n",
    "# 初始化存储每个折叠结果的列表\n",
    "test_tprs_micro = []\n",
    "test_aucs_micro = []\n",
    "test_mean_fpr_micro = np.linspace(0, 1, 100)\n",
    "test_aucs_95ci = []\n",
    "test_conf_matrices = []\n",
    "\n",
    "# 评估模型在测试集上的性能\n",
    "for model_name, model in tuned_models.items():\n",
    "    # 预测测试集上的概率\n",
    "    y_proba_test = model.predict_proba(X_test)\n",
    "    model_predictions[model_name] = y_proba_test[:, 1]\n",
    "    \n",
    "    # 计算ROC曲线和AUC\n",
    "    fpr, tpr, _ = roc_curve(Y_test, y_proba_test[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # 计算 micro 平均的ROC曲线和AUC\n",
    "    interp_tpr_micro = np.interp(test_mean_fpr_micro, fpr, tpr)\n",
    "    interp_tpr_micro[0] = 0.0  # 初始点为0\n",
    "    test_tprs_micro.append(interp_tpr_micro)\n",
    "    test_aucs_micro.append(roc_auc)\n",
    "    \n",
    "    # print(model.predict_proba(X_test))\n",
    "    # 设定新的阈值\n",
    "    threshold = 0.5\n",
    "\n",
    "    # 使用新的阈值进行预测\n",
    "    y_pred_test = (y_proba_test[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # 计算测试集上的性能指标\n",
    "    # y_pred_test = model.predict(X_test)\n",
    "    accuracy_test = accuracy_score(Y_test, y_pred_test)\n",
    "    f1_test = f1_score(Y_test, y_pred_test, average='weighted')\n",
    "\n",
    "    # 计算测试集上的混淆矩阵\n",
    "    cm = confusion_matrix(Y_test, y_pred_test, labels=np.unique(Y_train))\n",
    "    test_conf_matrices.append(cm)\n",
    "\n",
    "    # 计算sensitivity和specificity    \n",
    "    TP = cm[1][1]  # 真阳性的数量\n",
    "    FN = cm[1][0]  # 假阴性的数量\n",
    "    TN = cm[0][0]  # 真阴性的数量\n",
    "    FP = cm[0][1]  # 假阳性的数量\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    npv = TN / (TN + FN) if (TP + FN) != 0 else 0\n",
    "\n",
    "    # 计算95%置信区间\n",
    "    accuracy_ci = compute_bootstrap_ci(accuracy_score, Y_test, y_pred_test)\n",
    "    f1_ci = compute_bootstrap_ci(lambda y, y_hat: f1_score(y, y_hat, average='weighted'), Y_test, y_pred_test)\n",
    "    auc_ci = compute_bootstrap_ci(roc_auc_score, Y_test, y_proba_test[:, 1])\n",
    "\n",
    "    sensitivity_ci = compute_bootstrap_ci(sensitivity_metric, Y_test, y_pred_test)\n",
    "\n",
    "    specificity_ci = compute_bootstrap_ci(specificity_metric, Y_test, y_pred_test)\n",
    "    ppv_mean, ppv_ci = compute_bootstrap_ci(ppv_metric, Y_test, y_pred_test)\n",
    "    # print(f\"PPV: {ppv_mean}, 95%CI: {ppv_ci}\")\n",
    "\n",
    "    # 计算 NPV 的置信区间\n",
    "    npv_mean, npv_ci = compute_bootstrap_ci(npv_metric, Y_test, y_pred_test)\n",
    "\n",
    "    test_aucs_95ci.append(auc_ci[1])\n",
    "\n",
    "    # 输出模型的性能指标\n",
    "    print(f'{model_name} (Internal Test Set):')\n",
    "    # 输出指标及其95%置信区间\n",
    "    print(f'  Accuracy = {accuracy_test:.3f}, 95%CI: {accuracy_ci[1]}')\n",
    "    print(f'  AUC (micro) = {roc_auc:.3f}, 95%CI: {auc_ci[1]}')\n",
    "    print(f'  F1-score = {f1_test:.3f}, 95%CI: {f1_ci[1]}')\n",
    "    print(f'  Mean Sensitivity = {sensitivity_ci[0]:.3f}, 95%CI: {sensitivity_ci[1]}')\n",
    "    print(f'  Mean Specificity = {specificity_ci[0]:.3f}, 95%CI: {specificity_ci[1]}')\n",
    "    print(f'  PPV = {ppv:.3f}, 95%CI: {ppv_ci}')\n",
    "    print(f'  NPV = {npv:.3f}, 95%CI: {npv_ci}')\n",
    "    \n",
    "    # 存储结果\n",
    "    test_results[model_name] = {\n",
    "        'accuracy': accuracy_test,\n",
    "        'auc_micro': roc_auc,\n",
    "        'f1_score': f1_test,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv,\n",
    "    }\n",
    "    \n",
    "\n",
    "# # 绘制主图\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i, model_name in enumerate(tuned_models.keys()):\n",
    "#     plt.plot(test_mean_fpr_micro, test_tprs_micro[i], lw=2, \n",
    "#              label=f'{model_name} (AUC = {test_aucs_micro[i]:.3f} 95%CI: {test_aucs_95ci[i]})')\n",
    "\n",
    "# # 添加放大子图（调整位置和大小）\n",
    "# ax_inset = inset_axes(plt.gca(), width=\"40%\", height=\"40%\", loc='lower right',\n",
    "#                       bbox_to_anchor=(0.1, 0.1, 0.9, 0.9),  # 控制位置和大小\n",
    "#                       bbox_transform=plt.gca().transAxes)\n",
    "\n",
    "# # 绘制放大区域的曲线（限定坐标范围）\n",
    "# for i, model_name in enumerate(tuned_models.keys()):\n",
    "#     ax_inset.plot(test_mean_fpr_micro, test_tprs_micro[i], lw=1.5, alpha=0.8)\n",
    "\n",
    "# # 设置放大区域的坐标范围（示例：FPR 0~0.2，TPR 0.8~1）\n",
    "# ax_inset.set_xlim(0, 0.3)\n",
    "# ax_inset.set_ylim(0.7, 1.0)\n",
    "# ax_inset.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "# # 添加放大区域的边框线\n",
    "# plt.gca().indicate_inset_zoom(ax_inset, edgecolor=\"gray\")\n",
    "\n",
    "# # 主图设置\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curves with Zoomed Inset')\n",
    "# plt.legend(loc='lower right', fontsize=8)\n",
    "# plt.xlim(0, 1)\n",
    "# plt.ylim(0, 1)\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制 micro 平均的ROC曲线图\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# for i, model_name in enumerate(tuned_models.keys()):\n",
    "#     plt.plot(test_mean_fpr_micro, test_tprs_micro[i], lw=2, label=f'{model_name} (AUC = {test_aucs_micro[i]:.3f} 95%CI: {test_aucs_95ci[i]})')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curves (Test Set)')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.gca().set_aspect('equal', adjustable='box')\n",
    "# plt.xlim(0, 1)  # 设置 x 轴范围\n",
    "# plt.ylim(0, 1)  # 设置 y 轴范围\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制每个模型的混淆矩阵图像\n",
    "# n_models = len(tuned_models)  # 模型数量\n",
    "# n_cols = 5  # 每行显示的模型数量\n",
    "# n_rows = (n_models + n_cols - 1) // n_cols  # 计算行数\n",
    "\n",
    "# plt.figure(figsize=(20, 4 * n_rows))  # 调整整体图像大小\n",
    "\n",
    "# for i, (model_name, conf_matrix) in enumerate(zip(tuned_models.keys(), test_conf_matrices)):\n",
    "#     plt.subplot(n_rows, n_cols, i + 1)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.unique(Y_train))\n",
    "#     disp.plot(cmap=plt.cm.Blues, ax=plt.gca(), colorbar=False)\n",
    "#     plt.title(f'Confusion Matrix for {model_name}')\n",
    "#     plt.xlabel('')  # 去掉x轴标签\n",
    "#     plt.ylabel('')  # 去掉y轴标签\n",
    "\n",
    "# plt.tight_layout()  # 调整子图间的布局以避免重叠\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制主图\n",
    "plt.figure(figsize=(8, 8),dpi=600)\n",
    "for i, model_name in enumerate(tuned_models.keys()):\n",
    "    # print(i)\n",
    "    plt.plot(test_mean_fpr_micro, test_tprs_micro[i], lw=2,\n",
    "             label=f'{model_name} (AUC = {test_aucs_micro[i]:.3f} 95%CI: {test_aucs_95ci[i][0]:.3f}-{test_aucs_95ci[i][1]:.3f})',)\n",
    "\n",
    "\n",
    "# 主图设置\n",
    "plt.xlabel('假阳性率')\n",
    "plt.ylabel('真正率')\n",
    "plt.title('ROC 曲线 (前瞻性队列)')\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "# 添加放大子图（调整位置和大小）\n",
    "ax_inset = inset_axes(plt.gca(), width=\"40%\", height=\"40%\", loc='lower right',\n",
    "                      bbox_to_anchor=(-0.32, 0.3,1.3,1.3),  # 控制位置和大小\n",
    "                      bbox_transform=plt.gca().transAxes)\n",
    "# 添加放大区域的边框线\n",
    "# plt.gca().indicate_inset_zoom(ax_inset, edgecolor=\"gray\")\n",
    "# 绘制放大区域的曲线（限定坐标范围）\n",
    "for i, model_name in enumerate(tuned_models.keys()):\n",
    "    ax_inset.plot(test_mean_fpr_micro, test_tprs_micro[i], lw=1.5, alpha=0.8)\n",
    "# 设置放大区域的坐标范围（示例：FPR 0~0.2，TPR 0.8~1）\n",
    "ax_inset.set_xlim(0.0, 0.3)\n",
    "ax_inset.set_ylim(0.7, 1.0)\n",
    "ax_inset.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "bin_edges = [0,0.1,0.2,0.5,0.8,1.0]\n",
    "\n",
    "# 计算模型的校准曲线\n",
    "def custom_calibration_curve(y_true, y_prob, bins):\n",
    "    # 初始化存储校准曲线的数据\n",
    "    fraction_of_positives = []\n",
    "    mean_predicted_value = []\n",
    "    \n",
    "    # 将预测概率分入各个区间\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_start = bins[i]\n",
    "        bin_end = bins[i + 1]\n",
    "        \n",
    "        # 在这个区间内找到所有概率\n",
    "        # 如果是最后一个分箱，使用 <= 1.0 的条件\n",
    "        if i == len(bins) - 2:\n",
    "            bin_mask = (y_prob >= bin_start) & (y_prob <= bin_end)\n",
    "        else:\n",
    "            bin_mask = (y_prob >= bin_start) & (y_prob < bin_end)\n",
    "        bin_true = y_true[bin_mask]\n",
    "        bin_prob = y_prob[bin_mask]\n",
    "        \n",
    "        if len(bin_true) > 0:  # 确保该区间不为空\n",
    "            fraction_of_positives.append(np.mean(bin_true))  # 该区间的正类比例\n",
    "            mean_predicted_value.append(np.mean(bin_prob))  # 该区间的平均预测概率\n",
    "    \n",
    "    return np.array(mean_predicted_value), np.array(fraction_of_positives)\n",
    "\n",
    "# 计算每个模型的 Brier score\n",
    "brier_scores = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    # if model_name == 'stacking' or model_name == 'voting' or model_name == 'RandomForest' or model_name == 'XGBoost' or model_name == 'MLP':\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    brier_score = brier_score_loss(Y_test, y_prob)\n",
    "    brier_scores[model_name] = brier_score\n",
    "\n",
    "# 按 Brier score 升序排列模型\n",
    "# sorted_models = sorted(brier_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "# 创建图形对象\n",
    "plt.figure(figsize=(8, 8), dpi=600)\n",
    "\n",
    "# 设置颜色和线型\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(brier_scores.items())))\n",
    "# colors='blue'\n",
    "# linestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':', '-']\n",
    "\n",
    "# 绘制模型的校准曲线\n",
    "for (model_name, _), color in zip(brier_scores.items(), colors):\n",
    "    model = tuned_models[model_name]\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    # CalibrationDisplay.from_estimator(model, X_test, Y_test, n_bins=5, name=f'{model_name} (Brier: {brier_scores[model_name]:.4f})', ax=plt.gca(), color=color, linestyle='--')\n",
    "    model_brier = brier_score_loss(Y_test, y_prob)\n",
    "    prob_true, prob_pred = custom_calibration_curve(Y_test, y_prob, bin_edges)\n",
    "    print(model_name, prob_true.shape, prob_pred.shape)\n",
    "    # 使用插值方法对校准曲线进行平滑处理\n",
    "    spl = make_interp_spline(prob_true, prob_pred, k=2)  # k=3 是三次样条插值\n",
    "    smooth_x = np.linspace(prob_true.min(), prob_true.max(), 10)\n",
    "    smooth_y = spl(smooth_x)\n",
    "    stack_brier_ci = compute_bootstrap_ci(brier_score_loss, Y_test, y_prob)\n",
    "    plt.plot(smooth_x, smooth_y, label=f'{model_name} (Brier Score: {model_brier:.3f})', linestyle='-', marker='')\n",
    "    # 绘制完美校准的对角线\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('校准曲线')\n",
    "plt.xlabel('预测概率')\n",
    "plt.ylabel('真实频率')\n",
    "plt.legend(loc='best')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "# 显示图像\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各模型DCA曲线\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(Y_test))\n",
    "def calculate_net_benefit(y_true, y_pred_prob, thresholds):\n",
    "    net_benefit = []\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        net_benefit.append(tp / total_samples - (fp / total_samples) * (threshold / (1 - threshold)))\n",
    "    \n",
    "    return net_benefit\n",
    "\n",
    "thresholds = np.linspace(0, 1, 35)\n",
    "\n",
    "# selected_models = {\n",
    "#     # 'RandomForest': models['RandomForest'],\n",
    "#     # 'LogisticRegression': models['LogisticRegression'],\n",
    "#     'XGBoost': models['XGBoost']\n",
    "# }\n",
    "\n",
    "# 存储每个模型的净收益\n",
    "net_benefit_models = {}\n",
    "\n",
    "# 为每个基础模型计算预测概率和净收益\n",
    "for model_name, model in tuned_models.items():\n",
    "    # if model_name == 'stacking' or model_name == 'voting' or model_name == 'RandomForest' or model_name == 'XGBoost' or model_name == 'MLP':\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]  # 针对二分类，选取正类概率\n",
    "    net_benefit = calculate_net_benefit(Y_test, y_pred_prob, thresholds)\n",
    "    net_benefit_models[model_name] = net_benefit\n",
    "    \n",
    "\n",
    "# 计算全干预线的净收益\n",
    "full_intervention_net_benefit = []\n",
    "for threshold in thresholds:\n",
    "    net_benefit = (35 / len(Y_test)) - (73 / len(Y_test)) * (threshold / (1 - threshold))\n",
    "    full_intervention_net_benefit.append(net_benefit)\n",
    "    \n",
    "\n",
    "calibra_color = plt.cm.viridis(np.linspace(0, 1, len(tuned_models)))\n",
    "# 绘制 DCA 曲线\n",
    "plt.figure(figsize=(8, 8), dpi=600)\n",
    "for model_name, net_benefit in net_benefit_models.items():\n",
    "    plt.plot(thresholds, net_benefit, label=f\"{model_name}\")\n",
    "# plt.plot(thresholds, net_benefit_vote, label=\"VotingClassifier\", color='blue')\n",
    "# plt.plot(thresholds, net_benefit_stack, label=\"StackingClassifier\", color='red')\n",
    "plt.plot(thresholds, full_intervention_net_benefit, label=\"ALL\", linestyle='--')\n",
    "plt.axhline(0, color='grey', linestyle='--', label=\"None\")\n",
    "plt.ylim(-0.1, 0.4)\n",
    "plt.xlabel(\"阈值\")\n",
    "plt.ylabel(\"净收益\")\n",
    "plt.title(\"决策曲线分析\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bootstrap_ci(metric_func, y_true, y_pred, y_proba=None, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    使用 bootstrap 方法计算给定指标的置信区间.\n",
    "    \n",
    "    参数:\n",
    "        metric_func: 评估指标函数（如 accuracy_score, f1_score 等）\n",
    "        y_true: 测试集的真实标签\n",
    "        y_pred: 测试集的预测标签\n",
    "        y_proba: 测试集的预测概率（如果需要计算 AUC）\n",
    "        n_bootstraps: 重采样的次数\n",
    "        alpha: 显著性水平（0.05 表示 95% 置信区间）\n",
    "    \n",
    "    返回:\n",
    "        均值, (下限, 上限) 的置信区间\n",
    "    \"\"\"\n",
    "    bootstrapped_scores = []\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    # 引入重采样的部分\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = resample(np.arange(n_samples), replace=True, n_samples=n_samples)\n",
    "        \n",
    "        # 使用 iloc 来索引\n",
    "        y_true_sample = y_true.iloc[indices]\n",
    "        y_pred_sample = y_pred[indices]\n",
    "\n",
    "        # 如果有 y_proba\n",
    "        if y_proba is not None:\n",
    "            y_proba_sample = y_proba[indices]\n",
    "\n",
    "        # 计算指标\n",
    "        score = metric_func(y_true_sample, y_pred_sample)\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    \n",
    "    # 计算均值和置信区间\n",
    "    bootstrapped_scores = np.array(bootstrapped_scores)\n",
    "    lower = round(np.percentile(bootstrapped_scores, 100 * alpha / 2), 3)\n",
    "    upper = round(np.percentile(bootstrapped_scores, 100 * (1 - alpha / 2)), 3)\n",
    "    mean_score = np.mean(bootstrapped_scores)\n",
    "    return mean_score, (lower, upper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# 选择一个基模型进行SHAP分析，例如XGBoost\n",
    "\n",
    "stacking_model = tuned_models['stacking']\n",
    "model_to_explain = stacking_model.named_estimators_['XGBoost']  # XGBoost作为例子\n",
    "X_train_for_shap = X_train  # 用训练数据来计算SHAP值\n",
    "\n",
    "# 创建SHAP解释器\n",
    "explainer = shap.Explainer(model_to_explain, X_train_for_shap)\n",
    "\n",
    "# 计算SHAP值\n",
    "shap_values = explainer(X_train_for_shap)\n",
    "\n",
    "# 绘制SHAP值图\n",
    "shap.summary_plot(shap_values, X_train_for_shap)\n",
    "\n",
    "# 获取类别 1 的 SHAP 值\n",
    "shap_values_test = explainer.shap_values(X_test)\n",
    "# 绘制SHAP值总结图（Summary Plot）\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\", show=False)\n",
    "plt.title(\"X_train\")\n",
    "plt.xlabel('')  # 移除 x 轴标签避免x轴重叠\n",
    "plt.xlim(0,2)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "shap.summary_plot(shap_values_test, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"X_test\")\n",
    "plt.xlabel('')  \n",
    "plt.xlim(0,2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 stacking_model 是你已经训练好的 Stacking 模型\n",
    "# 假设 X_test 是测试集\n",
    "\n",
    "# 使用 TreeExplainer (如果是树模型的话) 或 KernelExplainer (对于一般模型)\n",
    "# explainer = shap.TreeExplainer(stacking_model)  # 如果是树模型\n",
    "explainer = shap.KernelExplainer(stacking_model.predict_proba, X_train, output='probability')  # 对于其他模型\n",
    "\n",
    "# 计算 SHAP 值\n",
    "shap_values = explainer.shap_values(X_test)  # 对测试集进行 SHAP 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 假设你要分析 X_test 中的第一个样本\n",
    "sample_idx = 3  # 选择第一个\n",
    "# print(shap_values[1])\n",
    "# print(explainer.expected_value[1])\n",
    "sample_shap_values = shap_values[:, :, 1]  # 如果是二分类，1表示正类的 SHAP 值\n",
    "\n",
    "# print(sample_shap_values)\n",
    "# print(sample_shap_values[sample_idx])  # 打印期望值\n",
    "shap.force_plot(explainer.expected_value[1], sample_shap_values[sample_idx], X_A[featureImportance.index.values.tolist()].iloc[sample_idx], matplotlib=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1, 2, 2)\n",
    "shap.summary_plot(shap_values[:, :, 1], X_test, plot_type=\"bar\", show=False)\n",
    "# plt.title(\"X_test\")\n",
    "plt.xlabel('mean(|SHAP value|) \\n(average impact on model output magnitude)')  \n",
    "plt.xlim(0,0.2)\n",
    "plt.xticks(np.arange(0, 0.3, 0.1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './final_stats/prospective.xlsx'  # 替换为您的文件路径\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "# data = data.dropna()\n",
    "columns_to_keep = ['AGE', 'BMI', 'tPSA', 'fPSA', 'f/t', 'p2PSA',\n",
    "                   'PHI', 'Volume', 'PSAD', 'PHID', 'Blood Neutrophil',\n",
    "                   'Blood Lymphocyte', 'NLR', 'PI-RADS', 'Urinary leukocytes',\n",
    "                   'Gleason Score']\n",
    "X_test_save = data[columns_to_keep]\n",
    "Y_test = data['SPC']\n",
    "# X_train_save, X_test_save, Y_train_save, Y_test_save = train_test_split(X, Y, test_size=0.3, random_state=2024, stratify=Y)\n",
    "\n",
    "# 计算中位数\n",
    "# median1 = np.mean(y_train_prob_stack[Y_train == 0])\n",
    "# median2 = np.mean(y_train_prob_stack[Y_train == 1])\n",
    "# median2 = 0.9\n",
    "\n",
    "# 划分区间\n",
    "# def classify_interval(prob):\n",
    "#     if prob < median1:\n",
    "#         return 'low'\n",
    "#     elif median1 <= prob < median2:\n",
    "#         return 'intermediate'\n",
    "#     else:\n",
    "#         return 'high'\n",
    "\n",
    "\n",
    "# 创建训练集结果的 DataFrame\n",
    "# train_results_df = pd.DataFrame({\n",
    "#     'Gleason Score': X_train_save['Gleason Score'],\n",
    "#     'PSA': X_train_save['tPSA'],\n",
    "#     'PHID': X_train_save['PHID'],\n",
    "#     'PI-RADS': X_train_save['PI-RADS'],\n",
    "#     'Volume': X_train_save['Volume'],\n",
    "#     'probabilities': y_train_prob_stack,\n",
    "#     'labels': Y_train,\n",
    "# })\n",
    "save_model_name = 'stacking'\n",
    "# print(tuned_models)\n",
    "for model_name, model in tuned_models.items():\n",
    "    if model_name == save_model_name:\n",
    "        print(model_name)\n",
    "        y_test_prob_save = model.predict_proba(X_test)[:, 1]\n",
    "        # 创建测试集结果的 DataFrame\n",
    "        test_results_df = pd.DataFrame({\n",
    "            'Gleason Score': X_test_save['Gleason Score'],\n",
    "            'PSA' : X_test_save['tPSA'],\n",
    "            'PHI': X_test_save['PHI'],\n",
    "            'PSAD': X_test_save['PSAD'],\n",
    "            'PHID': X_test_save['PHID'],\n",
    "            'PI-RADS': X_test_save['PI-RADS'],\n",
    "            'Volume': X_test_save['Volume'],\n",
    "            'probabilities': y_test_prob_save,\n",
    "            'labels': Y_test,\n",
    "        })\n",
    "\n",
    "# 将训练集和测试集的DataFrame合并\n",
    "# results_df = pd.concat([train_results_df, test_results_df], ignore_index=True)\n",
    "# print(results_df.shape)\n",
    "# 将合并后的DataFrame保存到CSV文件\n",
    "test_results_df.to_excel('./final_stats/stacking_test_predictions_new.xlsx', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "# 1. 获取模型的预测概率\n",
    "y_probs = tuned_models['stacking'].predict_proba(X_train)[:, 1]  # 取正类的概率\n",
    "y_true = Y_train  # 真实标签\n",
    "\n",
    "# 2. 定义不同的阈值\n",
    "thresholds = np.linspace(0, 1, 1000)  # 生成多个阈值\n",
    "\n",
    "# 3. 存储性能指标\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_probs >= thresh).astype(int)  # 应用阈值进行分类\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # 计算各项指标\n",
    "    # avoid_bispsy = \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_probs)  # AUC 与阈值无关\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)  # 召回率\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # 特异度\n",
    "    ppv = precision_score(y_true, y_pred) if (tp + fp) > 0 else 0  # 阳性预测值 PPV\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # 阴性预测值 NPV\n",
    "\n",
    "    results.append([thresh, accuracy, auc, f1, sensitivity, specificity, ppv, npv])\n",
    "\n",
    "# 4. 转换为 DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Threshold\", \"Accuracy\", \"AUC\", \"F1-score\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\"])\n",
    "\n",
    "# 5. 查找 NPV = 100% 的最佳阈值\n",
    "df_npv_100 = df_results[df_results[\"NPV\"] == 1.0]\n",
    "\n",
    "if not df_npv_100.empty:\n",
    "    # 选择 Accuracy 最高的行作为最优阈值\n",
    "    best_row = df_npv_100.loc[df_npv_100[\"Accuracy\"].idxmax()]\n",
    "    best_threshold = best_row[\"Threshold\"]\n",
    "\n",
    "    print(f\"最佳阈值 (NPV=100%): {best_threshold:.4f}\")\n",
    "    print(best_row)\n",
    "else:\n",
    "    print(\"找不到 NPV=100% 的阈值，尝试放宽条件。\")\n",
    "\n",
    "\n",
    "df_npv_95 = df_results[(0.96 >=  df_results[\"NPV\"]) & (df_results[\"NPV\"] >= 0.95)]\n",
    "\n",
    "if not df_npv_95.empty:\n",
    "    # 选择 Accuracy 最高的行作为最优阈值\n",
    "    best_row = df_npv_95.loc[df_npv_95[\"Accuracy\"].idxmax()]\n",
    "    best_threshold = best_row[\"Threshold\"]\n",
    "\n",
    "    print(f\"最佳阈值 (NPV>=95%): {best_threshold:.4f}\")\n",
    "    print(best_row)\n",
    "else:\n",
    "    print(\"找不到 NPV>=95% 的阈值，尝试放宽条件。\")\n",
    "    \n",
    "df_ppv_100 = df_results[df_results[\"PPV\"] == 1.0]\n",
    "\n",
    "if not df_ppv_100.empty:\n",
    "    # 选择 Accuracy 最高的行作为最优阈值\n",
    "    best_row = df_ppv_100.loc[df_ppv_100[\"Accuracy\"].idxmax()]\n",
    "    best_threshold = best_row[\"Threshold\"]\n",
    "\n",
    "    print(f\"最佳阈值 (PPV=100%): {best_threshold:.4f}\")\n",
    "    print(best_row)\n",
    "else:\n",
    "    print(\"找不到 PPV=100% 的阈值，尝试放宽条件。\")\n",
    "\n",
    "\n",
    "df_ppv_95 = df_results[(0.96 >=  df_results[\"PPV\"]) & (df_results[\"PPV\"] >= 0.95)]\n",
    "\n",
    "if not df_ppv_95.empty:\n",
    "    # 选择 Accuracy 最高的行作为最优阈值\n",
    "    best_row = df_ppv_95.loc[df_ppv_95[\"Accuracy\"].idxmax()]\n",
    "    best_threshold = best_row[\"Threshold\"]\n",
    "\n",
    "    print(f\"最佳阈值 (PPV>=95%): {best_threshold:.4f}\")\n",
    "    print(best_row)\n",
    "else:\n",
    "    print(\"找不到 PPV=95% 的阈值，尝试放宽条件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_test_psa_leq_10 = X_B[X_B['tPSA'] <= 10]\n",
    "X_test_psa_gt_10 = X_B[X_B['tPSA'] > 10]\n",
    "X_test_psa_leq_10_transformed = preprocessor.transform(X_test_psa_leq_10)\n",
    "X_test_psa_gt_10_transformed = preprocessor.transform(X_test_psa_gt_10)\n",
    "# 4. 将结果转换为 DataFrame\n",
    "X_test_psa_leq_10_scaled_df = pd.DataFrame(X_test_psa_leq_10_transformed, columns=continuous_features + non_trans_features)\n",
    "X_test_psa_gt_10_scaled_df = pd.DataFrame(X_test_psa_gt_10_transformed, columns=continuous_features + non_trans_features)\n",
    "\n",
    "# 5. 确保列的顺序和原始数据一致\n",
    "X_test_psa_leq_10 = X_test_psa_leq_10_scaled_df[continuous_features + non_trans_features]\n",
    "X_test_psa_gt_10 = X_test_psa_gt_10_scaled_df[continuous_features + non_trans_features]\n",
    "# 1. 根据 tPSA 划分 X_test 和 Y_test\n",
    "Y_test_psa_leq_10 = y_B[X_B['tPSA'] <= 10]\n",
    "Y_test_psa_gt_10 = y_B[X_B['tPSA'] > 10]\n",
    "\n",
    "X_test_psa_leq_10 = X_test_psa_leq_10.loc[:,featureImportance.index]\n",
    "X_test_psa_gt_10 = X_test_psa_gt_10.loc[:,featureImportance.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_psa_gt_10\n",
    "Y_test = Y_test_psa_gt_10\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "# 初始化存储每个模型结果的字典\n",
    "test_results = {}\n",
    "\n",
    "# 初始化存储每个折叠结果的列表\n",
    "test_tprs_micro = []\n",
    "test_aucs_micro = []\n",
    "test_mean_fpr_micro = np.linspace(0, 1, 100)\n",
    "test_aucs_95ci = []\n",
    "test_conf_matrices = []\n",
    "\n",
    "# 评估模型在测试集上的性能\n",
    "for model_name, model in tuned_models.items():\n",
    "    if model_name == 'voting':\n",
    "        # 预测测试集上的概率\n",
    "        y_proba_test = model.predict_proba(X_test)\n",
    "        model_predictions[model_name] = y_proba_test[:, 1]\n",
    "        \n",
    "        # 计算ROC曲线和AUC\n",
    "        fpr, tpr, _ = roc_curve(Y_test, y_proba_test[:, 1])\n",
    "        # print(fpr, tpr)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # 计算 micro 平均的ROC曲线和AUC\n",
    "        interp_tpr_micro = np.interp(test_mean_fpr_micro, fpr, tpr)\n",
    "        interp_tpr_micro[0] = 0.0  # 初始点为0\n",
    "        test_tprs_micro.append(interp_tpr_micro)\n",
    "        test_aucs_micro.append(roc_auc)\n",
    "        \n",
    "        # print(model.predict_proba(X_test))\n",
    "        # 设定新的阈值\n",
    "        threshold = 0.5\n",
    "\n",
    "        # 使用新的阈值进行预测\n",
    "        y_pred_test = (y_proba_test[:, 1] >= threshold).astype(int)\n",
    "        \n",
    "        # 计算测试集上的性能指标\n",
    "        # y_pred_test = model.predict(X_test)\n",
    "        accuracy_test = accuracy_score(Y_test, y_pred_test)\n",
    "        f1_test = f1_score(Y_test, y_pred_test, average='weighted')\n",
    "\n",
    "        # 计算测试集上的混淆矩阵\n",
    "        cm = confusion_matrix(Y_test, y_pred_test, labels=np.unique(Y_train))\n",
    "        test_conf_matrices.append(cm)\n",
    "\n",
    "        # 计算sensitivity和specificity    \n",
    "        TP = cm[1][1]  # 真阳性的数量\n",
    "        FN = cm[1][0]  # 假阴性的数量\n",
    "        TN = cm[0][0]  # 真阴性的数量\n",
    "        FP = cm[0][1]  # 假阳性的数量\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "        ppv = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        npv = TN / (TN + FN) if (TP + FN) != 0 else 0\n",
    "\n",
    "        # 计算95%置信区间\n",
    "        accuracy_ci = compute_bootstrap_ci(accuracy_score, Y_test, y_pred_test)\n",
    "        f1_ci = compute_bootstrap_ci(lambda y, y_hat: f1_score(y, y_hat, average='weighted'), Y_test, y_pred_test)\n",
    "        auc_ci = compute_bootstrap_ci(roc_auc_score, Y_test, y_proba_test[:, 1])\n",
    "\n",
    "        sensitivity_ci = compute_bootstrap_ci(sensitivity_metric, Y_test, y_pred_test)\n",
    "\n",
    "        specificity_ci = compute_bootstrap_ci(specificity_metric, Y_test, y_pred_test)\n",
    "        ppv_mean, ppv_ci = compute_bootstrap_ci(ppv_metric, Y_test, y_pred_test)\n",
    "        # print(f\"PPV: {ppv_mean}, 95%CI: {ppv_ci}\")\n",
    "\n",
    "        # 计算 NPV 的置信区间\n",
    "        npv_mean, npv_ci = compute_bootstrap_ci(npv_metric, Y_test, y_pred_test)\n",
    "\n",
    "        test_aucs_95ci.append(auc_ci[1])\n",
    "\n",
    "        # 输出模型的性能指标\n",
    "        print(f'{model_name} (Internal Test Set):')\n",
    "        # 输出指标及其95%置信区间\n",
    "        print(f'  Accuracy = {accuracy_test:.3f}, 95%CI: {accuracy_ci[1]}')\n",
    "        print(f'  AUC (micro) = {roc_auc:.3f}, 95%CI: {auc_ci[1]}')\n",
    "        print(f'  F1-score = {f1_test:.3f}, 95%CI: {f1_ci[1]}')\n",
    "        print(f'  Mean Sensitivity = {sensitivity_ci[0]:.3f}, 95%CI: {sensitivity_ci[1]}')\n",
    "        print(f'  Mean Specificity = {specificity_ci[0]:.3f}, 95%CI: {specificity_ci[1]}')\n",
    "        print(f'  PPV = {ppv:.3f}, 95%CI: {ppv_ci}')\n",
    "        print(f'  NPV = {npv:.3f}, 95%CI: {npv_ci}')\n",
    "        \n",
    "        # 存储结果\n",
    "        test_results[model_name] = {\n",
    "            'accuracy': accuracy_test,\n",
    "            'auc_micro': roc_auc,\n",
    "            'f1_score': f1_test,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'PPV': ppv,\n",
    "            'NPV': npv,\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './final_stats/prospective.xlsx'  # 替换为您的文件路径\n",
    "\n",
    "data = pd.read_excel(file_path)\n",
    "# data = data.dropna()\n",
    "columns_to_keep = ['AGE', 'BMI', 'tPSA', 'fPSA', 'f/t', 'p2PSA',\n",
    "                   'PHI', 'Volume', 'PSAD', 'PHID', 'Blood Neutrophil',\n",
    "                   'Blood Lymphocyte', 'NLR', 'PI-RADS', 'Urinary leukocytes',\n",
    "                   'Gleason Score']\n",
    "\n",
    "# print(data)\n",
    "Y_test = data['SPC']\n",
    "save_model_name = 'voting'\n",
    "Y_test = Y_test[X_B['tPSA'] > 10]\n",
    "data = data[columns_to_keep]\n",
    "X_test_save = data[X_B['tPSA'] > 10]\n",
    "\n",
    "print(X_test_save, X_test_psa_gt_10.head())\n",
    "# print(X_test_save.head(),X_test.shape)\n",
    "# print(tuned_models)\n",
    "for model_name, model in tuned_models.items():\n",
    "    if model_name == save_model_name:\n",
    "        print(model_name)\n",
    "        y_test_prob_save = model.predict_proba(X_test_psa_gt_10)[:, 1]\n",
    "        # 创建测试集结果的 DataFrame\n",
    "        test_results_df = pd.DataFrame({\n",
    "            'Gleason Score': X_test_save['Gleason Score'],\n",
    "            'PSA' : X_test_save['tPSA'],\n",
    "            'PHID': X_test_save['PHID'],\n",
    "            'PI-RADS': X_test_save['PI-RADS'],\n",
    "            'Volume': X_test_save['Volume'],\n",
    "            'probabilities': y_test_prob_save,\n",
    "            'labels': Y_test,\n",
    "        })\n",
    "\n",
    "# 将训练集和测试集的DataFrame合并\n",
    "# results_df = pd.concat([train_results_df, test_results_df], ignore_index=True)\n",
    "# print(results_df.shape)\n",
    "# 将合并后的DataFrame保存到CSV文件\n",
    "test_results_df.to_excel('./final_stats/voting_test_predictions_tpsa>10.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_ci(y_true, y_pred_proba):\n",
    "    # Bootstrap法计算置信区间\n",
    "    n_iterations = 1000\n",
    "    auc_scores = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # 重采样数据\n",
    "        indices = resample(np.arange(len(y_true)), n_samples=len(y_true))\n",
    "        y_true_resampled = y_true[indices]\n",
    "        y_pred_resampled = y_pred_proba[indices]\n",
    "        \n",
    "        # 计算每次采样的AUC\n",
    "        auc_scores.append(roc_auc_score(y_true_resampled, y_pred_resampled))\n",
    "\n",
    "    # 计算置信区间\n",
    "    lower = round(np.percentile(auc_scores, 2.5),3)\n",
    "    upper = round(np.percentile(auc_scores, 97.5),3)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"95% Confidence Interval: ({lower}, {upper})\")\n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "file_path = './final_stats/prospective.xlsx'  # 替换为您的文件路径\n",
    "\n",
    "external_data = pd.read_excel(file_path)\n",
    "external_data = external_data.dropna()\n",
    "columns_to_keep = ['AGE', 'BMI', 'tPSA', 'fPSA', 'f/t', 'p2PSA',\n",
    "                   'PHI', 'Volume', 'PSAD', 'PHID', 'Blood Neutrophil',\n",
    "                   'Blood Lymphocyte', 'NLR', 'PI-RADS', 'Urinary leukocytes']\n",
    "external_X_test = external_data[columns_to_keep]\n",
    "# external_X_test = external_data[featureImportance.index.values.tolist()]\n",
    "external_Y_test = external_data['SPC']\n",
    "\n",
    "# 假设外部验证集为 external_test\n",
    "scaled_features_external = scaler.transform(external_X_test)\n",
    "\n",
    "# 将归一化后的数据转换为 DataFrame，保持列名一致\n",
    "external_X_test = pd.DataFrame(scaled_features_external, columns=external_X_test.columns)\n",
    "\n",
    "external_X_test = external_X_test[featureImportance.index.values.tolist()]\n",
    "# print(external_X_test)\n",
    "\n",
    "# XGBoost模型\n",
    "# 提取 XGBoost 模型\n",
    "xgboost_model = models['XGBoost']\n",
    "# 计算 XGBoost 模型的预测概率\n",
    "y_proba_test_xgboost = xgboost_model.predict_proba(external_X_test)[:, 1]  # 正类概率\n",
    "\n",
    "# 计算 XGBoost 模型的 ROC 曲线和 AUC\n",
    "fpr_xgboost, tpr_xgboost, _ = roc_curve(external_Y_test, y_proba_test_xgboost)\n",
    "roc_auc_xgboost = auc(fpr_xgboost, tpr_xgboost)\n",
    "\n",
    "# 应用自定义阈值\n",
    "y_pred_test_xgboost = (y_proba_test_xgboost >= custom_threshold).astype(int)\n",
    "test_conf_matrices = []\n",
    "# 计算 XGBoost 模型的混淆矩阵\n",
    "cm_xgboost = confusion_matrix(external_Y_test, y_pred_test_xgboost, labels=np.unique(external_Y_test))\n",
    "test_conf_matrices.append(cm_xgboost)\n",
    "\n",
    "# 计算 XGBoost 的评估指标\n",
    "TP = cm_xgboost[1, 1]\n",
    "FN = cm_xgboost[1, 0]\n",
    "TN = cm_xgboost[0, 0]\n",
    "FP = cm_xgboost[0, 1]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "sensitivity_xgboost = TP / (TP + FN)\n",
    "specificity_xgboost = TN / (TN + FP)\n",
    "accuracy_xgboost = accuracy_score(external_Y_test, y_pred_test_xgboost)\n",
    "f1_xgboost = f1_score(external_Y_test, y_pred_test_xgboost, average='weighted')\n",
    "# xg_auc_ci = compute_bootstrap_ci(roc_auc_score, external_Y_test, y_pred_test_xgboost, y_proba=y_proba_test_xgboost)\n",
    "xg_auc_ci = auc_ci(external_Y_test, y_proba_test_xgboost)\n",
    "xg_accuracy_ci = compute_bootstrap_ci(accuracy_score, external_Y_test, y_pred_test_xgboost)\n",
    "xg_f1_ci = compute_bootstrap_ci(lambda y, y_hat: f1_score(y, y_hat, average='weighted'), external_Y_test, y_pred_test_xgboost)\n",
    "xg_sensitivity_ci = compute_bootstrap_ci(lambda y, y_hat: np.mean(np.diag(confusion_matrix(y, y_hat)) / np.sum(confusion_matrix(y, y_hat), axis=1)),\n",
    "                                    external_Y_test, y_pred_test_xgboost)\n",
    "xg_specificity_ci = compute_bootstrap_ci(specificity_metric, external_Y_test, y_pred_test_xgboost)\n",
    "xg_ppv_mean, xg_ppv_ci = compute_bootstrap_ci(ppv_metric, external_Y_test, y_pred_test_xgboost)\n",
    "xg_npv_mean, xg_npv_ci = compute_bootstrap_ci(npv_metric, external_Y_test, y_pred_test_xgboost)\n",
    "# 打印 XGBoost 的结果\n",
    "model_name_xgboost = 'XGBoost'\n",
    "print(f'{model_name_xgboost} (External Test Set):')\n",
    "print(f'Accuracy = {accuracy_xgboost:.4f} 95%CI: {xg_accuracy_ci[1]}')\n",
    "print(f'F1-score = {f1_xgboost:.4f} 95%CI: {xg_f1_ci[1]}')\n",
    "print(f'AUC (micro) = {roc_auc_xgboost:.4f} 95%CI: {xg_auc_ci}')\n",
    "print(f'Sensitivity = {sensitivity_xgboost:.4f} 95%CI: {xg_sensitivity_ci[1]}')\n",
    "print(f'Specificity = {specificity_xgboost:.4f} 95%CI: {xg_specificity_ci[1]}')\n",
    "print(f'NPV = {NPV:.4f} 95%CI: {xg_npv_ci}')\n",
    "print(f'PPV = {PPV:.4f} 95%CI: {xg_ppv_ci}')\n",
    "print()\n",
    "\n",
    "model_names = []\n",
    "test_accuracies = []\n",
    "test_f1_scores = []\n",
    "test_aucs_micro = []\n",
    "test_sensitivities = []\n",
    "test_specificities = []\n",
    "# 存储结果\n",
    "model_names.append(model_name_xgboost)\n",
    "test_accuracies.append(accuracy_xgboost)\n",
    "test_f1_scores.append(f1_xgboost)\n",
    "test_aucs_micro.append(roc_auc_xgboost)\n",
    "test_sensitivities.append(sensitivity_xgboost)\n",
    "test_specificities.append(specificity_xgboost)\n",
    "\n",
    "# Stacking 模型\n",
    "# 计算 Stacking 模型的预测概率\n",
    "y_proba_test_stacking = stacking_model.predict_proba(external_X_test)[:, 1]  # 正类概率\n",
    "\n",
    "# 计算 Stacking 模型的 ROC 曲线和 AUC\n",
    "fpr_stacking, tpr_stacking, _ = roc_curve(external_Y_test, y_proba_test_stacking)\n",
    "roc_auc_stacking = auc(fpr_stacking, tpr_stacking)\n",
    "\n",
    "# 应用自定义阈值\n",
    "y_pred_test_stacking = (y_proba_test_stacking >= custom_threshold).astype(int)\n",
    "\n",
    "# 计算 Stacking 模型的混淆矩阵\n",
    "cm_stacking = confusion_matrix(external_Y_test, y_pred_test_stacking, labels=np.unique(external_Y_test))\n",
    "test_conf_matrices.append(cm_stacking)\n",
    "\n",
    "# 计算 Stacking 的评估指标\n",
    "TP = cm_stacking[1, 1]\n",
    "FN = cm_stacking[1, 0]\n",
    "TN = cm_stacking[0, 0]\n",
    "FP = cm_stacking[0, 1]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "sensitivity_stacking = TP / (TP + FN)\n",
    "specificity_stacking = TN / (TN + FP)\n",
    "accuracy_stacking = accuracy_score(external_Y_test, y_pred_test_stacking)\n",
    "f1_stacking = f1_score(external_Y_test, y_pred_test_stacking, average='weighted')\n",
    "# stack_auc_ci = compute_bootstrap_ci(roc_auc_score, external_Y_test, y_pred_test_stacking, y_proba=y_proba_test_stacking)\n",
    "stack_auc_ci =   auc_ci(external_Y_test, y_proba_test_stacking)\n",
    "stack_accuracy_ci = compute_bootstrap_ci(accuracy_score, external_Y_test, y_pred_test_stacking)\n",
    "stack_f1_ci = compute_bootstrap_ci(lambda y, y_hat: f1_score(y, y_hat, average='weighted'), external_Y_test, y_pred_test_stacking)\n",
    "\n",
    "stack_sensitivity_ci = compute_bootstrap_ci(sensitivity_metric, external_Y_test, y_pred_test_stacking)\n",
    "\n",
    "stack_specificity_ci = compute_bootstrap_ci(specificity_metric, external_Y_test, y_pred_test_stacking)\n",
    "stack_ppv_mean, stack_ppv_ci = compute_bootstrap_ci(ppv_metric, external_Y_test, y_pred_test_stacking)\n",
    "stack_npv_mean, stack_npv_ci = compute_bootstrap_ci(npv_metric, external_Y_test, y_pred_test_stacking)\n",
    "# 打印 Stacking 的结果\n",
    "model_name_stacking = 'Stacking'\n",
    "print(f'{model_name_stacking} (External Test Set):')\n",
    "print(f'Accuracy = {accuracy_stacking:.4f} 95%CI: {stack_accuracy_ci[1]}')\n",
    "print(f'F1-score = {f1_stacking:.4f} 95%CI: {stack_f1_ci[1]}')\n",
    "print(f'AUC (micro) = {roc_auc_stacking:.4f} 95%CI: {stack_auc_ci}')\n",
    "print(f'Sensitivity = {sensitivity_stacking:.4f} 95%CI: {stack_sensitivity_ci[1]}')\n",
    "print(f'Specificity = {specificity_stacking:.4f} 95%CI: {stack_specificity_ci[1]}')\n",
    "print(f'NPV = {NPV:.4f} 95%CI: {stack_npv_ci}')\n",
    "print(f'PPV = {PPV:.4f} 95%CI: {stack_ppv_ci}')\n",
    "print()\n",
    "\n",
    "# 存储结果\n",
    "model_names.append(model_name_stacking)\n",
    "test_accuracies.append(accuracy_stacking)\n",
    "test_f1_scores.append(f1_stacking)\n",
    "test_aucs_micro.append(roc_auc_stacking)\n",
    "test_sensitivities.append(sensitivity_stacking)\n",
    "test_specificities.append(specificity_stacking)\n",
    "\n",
    "# Voting 模型\n",
    "# Predict probabilities for the positive class on test data\n",
    "y_proba_test_voting = voting_model.predict_proba(external_X_test)[:, 1]  # 只选择正类的概率\n",
    "\n",
    "# 应用自定义阈值：根据阈值将概率转换为预测类别\n",
    "y_pred_test_voting_custom_threshold = (y_proba_test_voting >= custom_threshold).astype(int)\n",
    "\n",
    "# 计算 Voting 模型的 ROC 曲线和 AUC\n",
    "fpr_voting, tpr_voting, _ = roc_curve(external_Y_test, y_proba_test_voting)\n",
    "roc_auc_voting = auc(fpr_voting, tpr_voting)\n",
    "\n",
    "# 绘制 Voting 模型的 ROC 曲线\n",
    "# plt.plot(fpr_voting, tpr_voting, lw=2, label=f'Voting (AUC = {roc_auc_voting:.4f})')\n",
    "\n",
    "# 计算 Voting 模型的测试集准确率和 F1-score\n",
    "# y_pred_test_voting = voting_model.predict(X_test)\n",
    "y_pred_test_voting = y_pred_test_voting_custom_threshold\n",
    "accuracy_test_voting = accuracy_score(external_Y_test, y_pred_test_voting)\n",
    "f1_test_voting = f1_score(external_Y_test, y_pred_test_voting, average='weighted')\n",
    "\n",
    "# 计算 Voting 模型的混淆矩阵\n",
    "\n",
    "cm_voting = confusion_matrix(external_Y_test, y_pred_test_voting, labels=np.unique(external_Y_test))\n",
    "test_conf_matrices.append(cm_voting)\n",
    "\n",
    "\n",
    "TP = cm_voting[1, 1]  # 真阳性\n",
    "FN = cm_voting[1, 0]  # 假阴性\n",
    "TN = cm_voting[0, 0]  # 真阴性\n",
    "FP = cm_voting[0, 1]  # 假阳性\n",
    "# 计算sensitivity和specificity\n",
    "sensitivity_vote = TP / (TP + FN)\n",
    "specificity_vote = TN / (TN + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "# vote_auc_ci = compute_bootstrap_ci(roc_auc_score, external_Y_test, y_pred_test_voting, y_proba=y_proba_test_voting)\n",
    "vote_auc_ci = auc_ci(external_Y_test, y_proba_test_voting)\n",
    "vote_accuracy_ci = compute_bootstrap_ci(accuracy_score, external_Y_test, y_pred_test_voting)\n",
    "vote_f1_ci = compute_bootstrap_ci(lambda y, y_hat: f1_score(y, y_hat, average='weighted'), external_Y_test, y_pred_test_voting)\n",
    "vote_sensitivity_ci = compute_bootstrap_ci(sensitivity_metric, external_Y_test, y_pred_test_voting)\n",
    "vote_specificity_ci = compute_bootstrap_ci(specificity_metric, external_Y_test, y_pred_test_voting)\n",
    "vote_ppv_mean, vote_ppv_ci = compute_bootstrap_ci(ppv_metric, external_Y_test, y_pred_test_voting)\n",
    "vote_npv_mean, vote_npv_ci = compute_bootstrap_ci(npv_metric, external_Y_test, y_pred_test_voting)\n",
    "\n",
    "# 打印并存储 Voting 模型的结果\n",
    "model_name_voting = 'Voting'\n",
    "print(f'{model_name_voting} (External Test Set):')\n",
    "print(f'Accuracy = {accuracy_test_voting:.4f} 95%CI: {vote_accuracy_ci[1]}')\n",
    "print(f'F1-score = {f1_test_voting:.4f} 95%CI: {vote_f1_ci[1]}')\n",
    "print(f'AUC (micro) = {roc_auc_voting:.4f} 95%CI: {vote_auc_ci}')\n",
    "print(f'Sensitivity = {sensitivity_vote:.4f} 95%CI: {vote_sensitivity_ci[1]}')\n",
    "print(f'Specificity = {specificity_vote:.4f} 95%CI: {vote_specificity_ci[1]}')\n",
    "print(f'NPV = {vote_npv_mean:.4f} 95%CI: {vote_npv_ci}')\n",
    "print(f'PPV = {vote_ppv_mean:.4f} 95%CI: {vote_ppv_ci}')\n",
    "print()\n",
    "\n",
    "model_names.append(model_name_voting)\n",
    "test_accuracies.append(accuracy_test_voting)\n",
    "test_f1_scores.append(f1_test_voting)\n",
    "test_aucs_micro.append(roc_auc_voting)\n",
    "test_sensitivities.append(sensitivity)\n",
    "test_specificities.append(specificity)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Initialize figure for ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "# 绘制 Stacking 模型的 ROC 曲线\n",
    "plt.plot(fpr_stacking, tpr_stacking, lw=2, label=f'Stacking (AUC = {roc_auc_stacking:.3f} 95%CI: {stack_auc_ci})')\n",
    "\n",
    "# 绘制 Voting 模型的 ROC 曲线\n",
    "fpr_micro_voting, tpr_micro_voting, _ = roc_curve(external_Y_test, y_proba_test_voting)  # Voting 模型的 fpr 和 tpr\n",
    "roc_auc_micro_voting = auc(fpr_micro_voting, tpr_micro_voting)  # 计算 AUC\n",
    "plt.plot(fpr_micro_voting, tpr_micro_voting, lw=2, label=f'Voting (AUC = {roc_auc_voting:.3f} 95%CI: {vote_auc_ci})')\n",
    "\n",
    "# 绘制 XGBoost 模型的 ROC 曲线\n",
    "plt.plot(fpr_xgboost, tpr_xgboost, lw=2, label=f'XGBoost (AUC = {roc_auc_xgboost:.3f} 95%CI: {xg_auc_ci})')\n",
    "\n",
    "\n",
    "\n",
    "# 添加图形细节\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Stacking and Voting Models')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# 设置轴的范围\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# 显示最终的图\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "n_models = len(model_names)\n",
    "n_cols = 3  # 每行展示 3 个混淆矩阵\n",
    "n_rows = (n_models + n_cols - 1) // n_cols  # 计算需要的行数\n",
    "\n",
    "plt.figure(figsize=(20, 4 * n_rows))\n",
    "\n",
    "for i, (model_name, conf_matrix) in enumerate(zip(model_names, test_conf_matrices)):\n",
    "    plt.subplot(n_rows, n_cols, i + 1)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.unique(external_Y_test))\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=plt.gca(), colorbar=False)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model Names: \", model_names)\n",
    "print(\"Number of Models: \", len(model_names))\n",
    "print(\"Confusion Matrices: \", test_conf_matrices)\n",
    "print(\"Number of Confusion Matrices: \", len(test_conf_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(len(external_Y_test))\n",
    "def calculate_net_benefit(y_true, y_pred_prob, thresholds):\n",
    "    net_benefit = []\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        net_benefit.append(tp / total_samples - (fp / total_samples) * (threshold / (1 - threshold)))\n",
    "    \n",
    "    return net_benefit\n",
    "\n",
    "thresholds = np.linspace(0, 1, 30)\n",
    "\n",
    "selected_models = {\n",
    "    # 'RandomForest': models['RandomForest'],\n",
    "    # 'LogisticRegression': models['LogisticRegression'],\n",
    "    'CatBoost': models['CatBoost']\n",
    "}\n",
    "\n",
    "# 存储每个模型的净收益\n",
    "net_benefit_models = {}\n",
    "\n",
    "# 为每个基础模型计算预测概率和净收益\n",
    "for model_name, model in selected_models.items():\n",
    "    y_pred_prob = model.predict_proba(external_X_test)[:, 1]  # 针对二分类，选取正类概率\n",
    "    net_benefit = calculate_net_benefit(external_Y_test, y_pred_prob, thresholds)\n",
    "    net_benefit_models[model_name] = net_benefit\n",
    "    \n",
    "# 预测概率\n",
    "external_y_pred_prob_vote = voting_model.predict_proba(external_X_test)[:, 1]  # 针对二分类，选取正类概率\n",
    "external_y_pred_prob_stack = stacking_model.predict_proba(external_X_test)[:, 1]  # 针对二分类，选取正类概率\n",
    "\n",
    "\n",
    "# 计算净收益\n",
    "net_benefit_vote = calculate_net_benefit(external_Y_test, external_y_pred_prob_vote, thresholds)\n",
    "net_benefit_stack = calculate_net_benefit(external_Y_test, external_y_pred_prob_stack, thresholds)\n",
    "\n",
    "# 计算全干预线的净收益\n",
    "full_intervention_net_benefit = []\n",
    "for threshold in thresholds:\n",
    "    net_benefit = (35 / len(external_Y_test)) - (73 / len(external_Y_test)) * (threshold / (1 - threshold))\n",
    "    full_intervention_net_benefit.append(net_benefit)\n",
    "    \n",
    "\n",
    "\n",
    "# 绘制 DCA 曲线\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.plot(thresholds, net_benefit_stack, label=\"StackingClassifier\")\n",
    "plt.plot(thresholds, net_benefit_vote, label=\"VotingClassifier\")\n",
    "for model_name, net_benefit in net_benefit_models.items():\n",
    "    plt.plot(thresholds, net_benefit, label=f\"{model_name}\")\n",
    "plt.plot(thresholds, full_intervention_net_benefit, label=\"ALL\", color='grey', linestyle='--')\n",
    "plt.axhline(0, color='grey', linestyle='--', label=\"None\")\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.xlabel(\"Threshold Probability\")\n",
    "plt.ylabel(\"Net Benefit\")\n",
    "plt.title(\"Decision Curve Analysis (DCA)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 校准曲线\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "# 创建图形对象\n",
    "plt.figure(figsize=(8, 8))\n",
    "bin_edges = [0,0.1, 0.4, 0.6, 0.9,1.0]  # 自定义区间 \n",
    "bin_centers = [(bin_edges[i] + bin_edges[i+1]) / 2 for i in range(len(bin_edges) - 1)]  # 计算区间中心\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "# 计算模型的校准曲线\n",
    "def custom_calibration_curve(y_true, y_prob, bins):\n",
    "    # 初始化存储校准曲线的数据\n",
    "    fraction_of_positives = []\n",
    "    mean_predicted_value = []\n",
    "    \n",
    "    # 将预测概率分入各个区间\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_start = bins[i]\n",
    "        bin_end = bins[i + 1]\n",
    "        \n",
    "        # 在这个区间内找到所有概率\n",
    "        bin_mask = (y_prob >= bin_start) & (y_prob < bin_end)\n",
    "        bin_true = y_true[bin_mask]\n",
    "        bin_prob = y_prob[bin_mask]\n",
    "        \n",
    "        if len(bin_true) > 0:  # 确保该区间不为空\n",
    "            fraction_of_positives.append(np.mean(bin_true))  # 该区间的正类比例\n",
    "            mean_predicted_value.append(np.mean(bin_prob))  # 该区间的平均预测概率\n",
    "    \n",
    "    return np.array(mean_predicted_value), np.array(fraction_of_positives)\n",
    "\n",
    "# 绘制 stacking 模型的校准曲线\n",
    "stack_y_prob = stacking_model.predict_proba(external_X_test)[:, 1]\n",
    "stacking_model_brier = brier_score_loss(external_Y_test, stack_y_prob)\n",
    "prob_true, prob_pred = custom_calibration_curve(external_Y_test, stack_y_prob, bin_edges)\n",
    "\n",
    "# 使用插值方法对校准曲线进行平滑处理\n",
    "spl = make_interp_spline(prob_true, prob_pred, k=2)  # k=3 是三次样条插值\n",
    "smooth_x = np.linspace(prob_true.min(), prob_true.max(), 20)\n",
    "smooth_y = spl(smooth_x)\n",
    "stack_brier_ci = compute_bootstrap_ci(brier_score_loss, external_Y_test, stack_y_prob)\n",
    "plt.plot(smooth_x, smooth_y, label=f'Stacking (Brier Score: {stacking_model_brier:.3f} 95%CI: {stack_brier_ci[1]})', linestyle='-', marker='')\n",
    "\n",
    "# 绘制 voting 模型的校准曲线\n",
    "vote_y_prob = voting_model.predict_proba(external_X_test)[:, 1]\n",
    "voting_model_brier = brier_score_loss(external_Y_test, vote_y_prob)\n",
    "prob_true, prob_pred = custom_calibration_curve(external_Y_test, vote_y_prob, bin_edges)\n",
    "spl = make_interp_spline(prob_true, prob_pred, k=2)  # k=3 是三次样条插值\n",
    "smooth_x = np.linspace(prob_true.min(), prob_true.max(), 20)\n",
    "smooth_y = spl(smooth_x)\n",
    "vote_brier_ci = compute_bootstrap_ci(brier_score_loss, external_Y_test, vote_y_prob)\n",
    "plt.plot(smooth_x, smooth_y, label=f'Voting (Brier Score: {voting_model_brier:.3f} 95%CI: {vote_brier_ci[1]})', linestyle='-', marker='')\n",
    "\n",
    "\n",
    "xgmodel = selected_models['CatBoost']\n",
    "xgmodel_y_prob = model.predict_proba(external_X_test)[:, 1]  # 针对二分类，选取正类概率\n",
    "xgmodel_brier = brier_score_loss(external_Y_test, xgmodel_y_prob)\n",
    "prob_true, prob_pred = custom_calibration_curve(external_Y_test, xgmodel_y_prob, bin_edges)\n",
    "spl = make_interp_spline(prob_true, prob_pred, k=2)  # k=3 是三次样条插值\n",
    "smooth_x = np.linspace(prob_true.min(), prob_true.max(), 20)\n",
    "smooth_y = spl(smooth_x)\n",
    "xgmodel_brier_ci = compute_bootstrap_ci(brier_score_loss, external_Y_test, xgmodel_y_prob)\n",
    "plt.plot(smooth_x, smooth_y, label=f'CatBoost (Brier Score: {xgmodel_brier:.3f} 95%CI: {xgmodel_brier_ci[1]})', linestyle='-', marker='')\n",
    "\n",
    "# 绘制完美校准的对角线\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Calibration Curves Sorted by Brier Score')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# 设置坐标轴比例\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "\n",
    "# 显示图像\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部验证集Waterfall plot\n",
    "# 将训练集的预测概率和真实标签存储到DataFrame中\n",
    "# train_results_df = pd.DataFrame({\n",
    "#     'True_Label': Y_train,\n",
    "#     'Predicted_Probability': y_train_prob_vote\n",
    "# })\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "file_path = './final_stats/retrospective.xlsx'  # 替换为您的文件路径\n",
    "\n",
    "external_data = pd.read_excel(file_path)\n",
    "external_data = external_data.dropna()\n",
    "columns_to_keep = ['AGE', 'BMI', 'tPSA', 'fPSA', 'f/t', 'p2PSA',\n",
    "                   'PHI', 'Volume', 'PSAD', 'PHID', 'Blood Neutrophil',\n",
    "                   'Blood Lymphocyte', 'NLR', 'PI-RADS', 'Urinary leukocytes',\n",
    "                   'Gleason Score']\n",
    "external_X_test = external_data[columns_to_keep]\n",
    "\n",
    "# 将测试集的预测概率和真实标签存储到DataFrame中\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Gleason Score': external_X_test['Gleason Score'],\n",
    "    'PSA': external_X_test['tPSA'],\n",
    "    'PHID': external_X_test['PHID'],\n",
    "    'PI-RADS': external_X_test['PI-RADS'],\n",
    "    'Volume': external_X_test['Volume'],\n",
    "    'labels': external_Y_test,\n",
    "    'probabilities': external_y_pred_prob_stack,\n",
    "})\n",
    "\n",
    "# 将训练集和测试集的DataFrame合并\n",
    "# results_df = pd.concat([train_results_df, test_results_df], ignore_index=True)\n",
    "\n",
    "# 将合并后的DataFrame保存到CSV文件\n",
    "test_results_df.to_excel('./final_stats/internal_stack_predictions.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预测概率作为一个新特征与 X_test 拼接\n",
    "X_test_with_prob = X_test.copy()  # 保持原始特征不变\n",
    "X_test_with_prob['Predicted_Probability'] = y_test_prob_vote  # 将预测概率加入新的列\n",
    "\n",
    "# 查看拼接后的数据\n",
    "print(X_test_with_prob.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csPCa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
